{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Goal is to create a comprehensive dataset witht he history of D1 college football conferences and their membership\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Read in the csv files\n",
    "# df = pd.read_csv('data/cfb_d1_teams.csv')\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check csv files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory_path = 'TEMP/conference_data'\n",
    "\n",
    "problem_files = []\n",
    "\n",
    "# Loop through all CSV files in the directory\n",
    "for file in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    try:\n",
    "        # Attempt to read the file with utf-8 encoding\n",
    "        _ = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        problem_files.append(file)\n",
    "\n",
    "print(\"Problematic files:\", problem_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine all of the dirty conference data into a single dataframe\n",
    "\n",
    "import os\n",
    "\n",
    "# list files in the data directory\n",
    "files = os.listdir('TEMP\\conference_data')\n",
    "\n",
    "files\n",
    "\n",
    "# create a list of dataframes\n",
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(f'TEMP\\conference_data\\{file}')\n",
    "    df_list.append(df)\n",
    "\n",
    "# concatenate the list of dataframes into a single dataframe\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# Output the dataframe to a csv file\n",
    "df.to_csv('TEMP/cfb_conferences_raw.csv', index=False)\n",
    "\n",
    "### Valuecounts Instituation\n",
    "df['Institution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns\n",
    "\n",
    "# drop_col = ['Enrollment(Fall 2020)[21]', 'Unnamed: 9_level_0', 'Endowment',\n",
    "#        'Endowment(billions)[8]', 'AdmissionRate', 'Establishment',  \n",
    "#        'Endowment(millions)', 'Endowment (millions)', 'Endowment(millions) [4]',\n",
    "#         'Colors']\n",
    "\n",
    "# df.drop(drop_col, axis=1, inplace=True)\n",
    "\n",
    "df.columns\n",
    "\n",
    "keep_col = ['Institution', 'Location', 'Founded', 'Joined', 'Type', 'Enrollment',\n",
    "         'Nickname', 'Conference ID', 'Left', 'Year Joined', 'Joined[20]', 'conference id',\n",
    "         'Establishment', 'Joined SEC', 'Left SEC']\n",
    "\n",
    "# keep the columns we want\n",
    "df = df[keep_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the multiple joined, left and conference id columns\n",
    "\n",
    "df_cfb_updated = df\n",
    "\n",
    "# Consolidate 'Joined' columns\n",
    "df_cfb_updated['Consolidated Joined'] = df_cfb_updated[['Joined', 'Joined[20]', 'Joined SEC']].apply(\n",
    "    lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Consolidate 'Left' columns\n",
    "df_cfb_updated['Consolidated Left'] = df_cfb_updated[['Left', 'Left SEC']].apply(\n",
    "    lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# # Consolidate 'Conference ID' columns\n",
    "# df_cfb_updated['Consolidated Conference ID'] = df_cfb_updated[['Conference ID', 'conference id', \n",
    "#                 ]].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Drop the original columns to avoid redundancy\n",
    "df_cfb_updated.drop(columns=['Joined', 'Joined[20]', 'Joined SEC', 'Left', 'Left SEC'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the consolidated dataframe\n",
    "df_cfb_updated.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cfb_updated['Consolidated Conference ID'].value_counts()\n",
    "\n",
    "## Rename everything for simplicity\n",
    "df = df_cfb_updated\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns={'Institution': 'School', 'Consolidated Joined': 'Joined', 'Consolidated Left': 'Left',\n",
    "                     }, inplace=True)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Year Joined to Joined if Joined is null\n",
    "df['Joined'] = np.where(df['Joined'].isnull(), df['Year Joined'], df['Joined'])\n",
    "\n",
    "# Drop Year Joined\n",
    "df.drop(columns=['Year Joined'], inplace=True)\n",
    "\n",
    "# df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['School', 'Location', 'Founded', 'Type', 'Enrollment', 'Nickname',\n",
       "       'Conference ID', 'conference id', 'Establishment', 'Joined', 'Left'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 324 entries, 0 to 31\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   School         315 non-null    object \n",
      " 1   Location       299 non-null    object \n",
      " 2   Founded        288 non-null    object \n",
      " 3   Type           269 non-null    object \n",
      " 4   Enrollment     196 non-null    object \n",
      " 5   Nickname       314 non-null    object \n",
      " 6   Establishment  3 non-null      float64\n",
      " 7   Joined         324 non-null    object \n",
      " 8   Left           324 non-null    object \n",
      " 9   Conference     324 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 27.8+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Consolidate 'Conference ID' columns to new Conference column\n",
    "df['Conference'] = df[['Conference ID', 'conference id']].apply(\n",
    "    lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# drop the redundant columns\n",
    "df.drop(columns=['conference id', 'Conference ID'], inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               205\n",
       "Western Athletic Conference     43\n",
       "Missouri Valley Conference      37\n",
       "American                        14\n",
       "Border Conference                9\n",
       "American - Former                8\n",
       "Ivy League                       8\n",
       "Name: Conference, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## School value count\n",
    "df['School'].value_counts()\n",
    "\n",
    "df['Conference'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the dataframe to a csv file\n",
    "df.to_csv('data/cfb_conferences_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "# Sort the dataframe by Institution\n",
    "df.sort_values(by=['Institution'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output CLeaned Conference Data to CSV\n",
    "df.to_csv('data/cfb_conferences_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "\n",
    "df.columns\n",
    "\n",
    "col_keep = 'institution', 'location', 'founded', 'type', 'enrollment', 'nickname',\n",
    "       'conference id', 'joined', 'left']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove placeholder rows\n",
    "\n",
    "\n",
    "# strip the whitespace from the column names and lowercase them\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "df = df[df['institution'] != 'Institution']\n",
    "# df_cfb_cleaned = df_cfb_cleaned[df_cfb_cleaned[\"institution\"] != \"Institution\"]\n",
    "\n",
    "\n",
    "# colsoildate the 'conference id' and 'conferenceid' columns\n",
    "df['conference id'] = df['conference id'].fillna(df['conferenceid'])\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "# Clean up the conference id column, remove '- Former' and '- Current' from the conference names\n",
    "df['conference id'] = df['conference id'].str.replace('- Former', '')\n",
    "df['conference id'] = df['conference id'].str.replace('- Current', '')\n",
    "# strip the whitespace from the conference id column\n",
    "df['conference id'] = df['conference id'].str.strip()\n",
    "\n",
    "# Display the first few rows after removing the placeholders\n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "df.value_counts('conference id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop conferenceid column\n",
    "# df.drop(columns=['conferenceid'], inplace=True)\n",
    "\n",
    "# # rename the consolidated joined and left columns\n",
    "# df.rename(columns={'consolidated joined': 'joined', 'consolidated left': 'left'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('TEMP/cfb_conferences_cleaned.csv', index=False)\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the 'joined' and 'left' columns\n",
    "def clean_year(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    # Extract year using regex\n",
    "    match = re.search(r'(\\d{4})', value)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Apply the function to the 'joined' and 'left' columns\n",
    "df['joined_clean'] = df['joined'].apply(clean_year)\n",
    "df['left_clean'] = df['left'].apply(clean_year)\n",
    "\n",
    "# Display the cleaned columns\n",
    "df[['institution', 'conference id', 'joined_clean', 'left_clean']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter institutions with multiple entries\n",
    "multi_entry_institutions = df['institution'].value_counts()\n",
    "multi_entry_institutions = multi_entry_institutions[multi_entry_institutions > 1].index.tolist()\n",
    "\n",
    "# Extract rows corresponding to these institutions\n",
    "multi_entry_df = df[df['institution'].isin(multi_entry_institutions)].sort_values(by=['institution', 'joined_clean'])\n",
    "\n",
    "multi_entry_df[['institution', 'conference id', 'joined_clean', 'left_clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of institutions that have shifted conferences\n",
    "num_shifted_institutions = len(multi_entry_institutions)\n",
    "\n",
    "num_shifted_institutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many unique institutions are there?\n",
    "len(df['institution'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conference transitions\n",
    "transitions = []\n",
    "\n",
    "# Loop through each institution in the multi_entry dataframe\n",
    "for institution in multi_entry_institutions:\n",
    "    institution_data = multi_entry_df[multi_entry_df['institution'] == institution]\n",
    "    previous_conference = None\n",
    "    for _, row in institution_data.iterrows():\n",
    "        current_conference = row['conference id']\n",
    "        if previous_conference:\n",
    "            transitions.append((previous_conference, current_conference))\n",
    "        previous_conference = current_conference\n",
    "\n",
    "# Convert transitions into a pandas Series and count occurrences\n",
    "transition_counts = pd.Series(transitions).value_counts()\n",
    "\n",
    "transition_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename to df_cfb_cleaned\n",
    "df_cfb_cleaned = df\n",
    "\n",
    "# Extracting the numeric portion of 'joined' and 'left' columns\n",
    "df_cfb_cleaned['joined'] = df_cfb_cleaned['joined'].astype(str).str.extract('(\\d+)').astype(float)\n",
    "df_cfb_cleaned['left'] = df_cfb_cleaned['left'].astype(str).str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Determine the range of years in the dataset\n",
    "min_year = df_cfb_cleaned['joined'].min()\n",
    "max_year = max(df_cfb_cleaned['left'].max(), 2023)  # Considering 2023 as the current year if 'left' is NaN\n",
    "\n",
    "# Create a new DataFrame with rows for each year\n",
    "df_yearly = pd.DataFrame({'Year': range(int(min_year), int(max_year) + 1)})\n",
    "# Continue with the population of the dataframe, but handle NaN values in the 'joined' and 'left' columns\n",
    "\n",
    "# Populate the dataframe with member institutions for each conference and year\n",
    "for index, row in df_cfb_cleaned.iterrows():\n",
    "    # Skip rows with NaN in 'joined'\n",
    "    if pd.isna(row['joined']):\n",
    "        continue\n",
    "    \n",
    "    start_year = int(row['joined'])\n",
    "    end_year = int(row['left']) if pd.notna(row['left']) else int(max_year)\n",
    "    conference = row['conference id']\n",
    "    for year in range(start_year, end_year):\n",
    "        if conference not in df_yearly.columns:\n",
    "            df_yearly[conference] = [set() for _ in range(len(df_yearly))]\n",
    "        df_yearly.loc[df_yearly['Year'] == year, conference].apply(lambda x: x.add(row['institution']))\n",
    "\n",
    "# Convert sets to lists for better readability\n",
    "for col in df_yearly.columns:\n",
    "    if col != 'Year':\n",
    "        df_yearly[col] = df_yearly[col].apply(list)\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "df_yearly.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the dataframe to a csv file\n",
    "df_yearly.to_csv('TEMP/cfb_conferences_yearly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload the dataframe\n",
    "df_yearly = pd.read_csv('TEMP/cfb_conferences_yearly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "df_yearly.replace('[]', np.nan, inplace=True)\n",
    "\n",
    "df_yearly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# drop all the columns with '- Former' in the name\n",
    "df_yearly.drop(columns=df_yearly.columns[df_yearly.columns.str.contains('- Former')], inplace=True)\n",
    "\n",
    "# Remove the '- Current' suffix from the remaining column names\n",
    "df_yearly.columns = df_yearly.columns.str.replace('- Current', '')\n",
    "df_yearly.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save cleaned dataframe to csv\n",
    "df_yearly.to_csv('TEMP/cfb_conferences_yearly_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Data Tranformation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Line Chart\n",
    "# Read the cleaned yearly CSV file into a DataFrame\n",
    "df_yearly_cleaned = df_yearly\n",
    "\n",
    "# Compute the number of members for each conference in every year\n",
    "df_member_counts = df_yearly_cleaned.drop('Year', axis=1).applymap(lambda x: len(eval(x)) if not pd.isna(x) else 0)\n",
    "\n",
    "# Plot a line chart\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for col in df_member_counts.columns:\n",
    "    plt.plot(df_yearly_cleaned['Year'], df_member_counts[col], label=col, marker='o')\n",
    "\n",
    "plt.title('Number of Conference Members Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Members')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transition matrix to visualize movement between conferences\n",
    "# Initialize a dictionary to capture transitions between conferences\n",
    "transition_dict = {}\n",
    "\n",
    "# Extract unique conferences from the data\n",
    "conferences = df_yearly_cleaned.columns.drop('Year').tolist()\n",
    "\n",
    "# Initialize transition dictionary with zeros\n",
    "for conf1 in conferences:\n",
    "    transition_dict[conf1] = {}\n",
    "    for conf2 in conferences:\n",
    "        transition_dict[conf1][conf2] = 0\n",
    "\n",
    "# Recalculate transitions without using eval\n",
    "previous_row = None\n",
    "for _, row in df_yearly_cleaned.iterrows():\n",
    "    if previous_row is not None:\n",
    "        for conf in conferences:\n",
    "            current_teams = row[conf]\n",
    "            if pd.isna(current_teams):\n",
    "                current_teams = []\n",
    "            for team in current_teams:\n",
    "                # Check which conference the team was in the previous year\n",
    "                for prev_conf in conferences:\n",
    "                    prev_teams = previous_row[prev_conf]\n",
    "                    if pd.isna(prev_teams):\n",
    "                        prev_teams = []\n",
    "                    if team in prev_teams:\n",
    "                        transition_dict[prev_conf][conf] += 1\n",
    "                        break\n",
    "    previous_row = row\n",
    "\n",
    "# Convert transition dict to a dataframe\n",
    "df_transition = pd.DataFrame(transition_dict)\n",
    "\n",
    "# Display the transition dataframe\n",
    "df_transition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the transition dataframe to a csv file\n",
    "df_transition.to_csv('TEMP/cfb_conferences_transition.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the restructured data\n",
    "df_restructured = pd.DataFrame(columns=['Institution', 'Year', 'Conference'])\n",
    "\n",
    "# Iterate over each row in the cleaned dataframe\n",
    "for index, row in df_cfb_cleaned.iterrows():\n",
    "    # Determine the start and end years based on 'joined' and 'left' columns\n",
    "    try:\n",
    "        start_year = int(row['joined'])\n",
    "        # If 'left' is NaN, use current year (2023) as the end year\n",
    "        end_year = int(row['left']) if pd.notna(row['left']) else 2023\n",
    "        for year in range(start_year, end_year):\n",
    "            df_restructured = df_restructured.append({\n",
    "                'Institution': row['institution'],\n",
    "                'Year': year,\n",
    "                'Conference': row['conference id']\n",
    "            }, ignore_index=True)\n",
    "    except:\n",
    "        # Handle rows where 'joined' or 'left' might not be valid integers\n",
    "        continue\n",
    "\n",
    "# Display the first few rows of the restructured dataframe\n",
    "df_restructured.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cfb_updated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overwirtes the dataframe with the cleaned data\n",
    "df_cfb_updated.to_csv('TEMP/cfb_conferences_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google api\n",
    "\n",
    "API_KEY = 'AIzaSyCnVFNJ7I-Muja5_9tmWiP37nxLnmMrUQw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "cfb_teams_df = pd.read_csv('data/cfb_d1_teams.csv')\n",
    "cfb_teams_df.rename(columns={'State[2]': 'State'}, inplace=True)\n",
    "\n",
    "\n",
    "BASE_URL = \"https://maps.googleapis.com/maps/api/geocode/json?address=\"\n",
    "\n",
    "def get_coordinates(city, state):\n",
    "    \"\"\"\n",
    "    Fetches the coordinates using the Google Places API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(BASE_URL + f\"{city},+{state}&key={API_KEY}\")\n",
    "        if response.status_code == 200:\n",
    "            json_result = response.json()\n",
    "            if json_result['status'] == 'OK':\n",
    "                location = json_result['results'][0]['geometry']['location']\n",
    "                return location['lat'], location['lng']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {city}, {state}. Error: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Fetch coordinates for each team and add to dataframe\n",
    "cfb_teams_df['Latitude'], cfb_teams_df['Longitude'] = zip(*cfb_teams_df.apply(lambda row: get_coordinates(row['City'], row['State']), axis=1))\n",
    "\n",
    "# Save the updated dataframe\n",
    "cfb_teams_df.to_csv('cfb_d1_teams_with_coordinates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of conferences\n",
    "\n",
    "## Current conferences - Data gathered 9/3/2023\n",
    "\n",
    "# Clean up the conference names\n",
    "# Remove anything in brackets\n",
    "import re\n",
    "\n",
    "def clean_conference_name(conf_name):\n",
    "    \"\"\"Function to remove appended citation/reference indicators from conference names.\"\"\"\n",
    "    return re.sub(r'\\[.*?\\]', '', conf_name).strip()\n",
    "\n",
    "# Cleaning up conference names\n",
    "cfb_teams_df['Currentconference'] = cfb_teams_df['Currentconference'].apply(clean_conference_name)\n",
    "\n",
    "# Display unique conference names after cleanup\n",
    "cfb_teams_df['Currentconference'].unique()\n",
    "\n",
    "cfb_teams_df['Currentconference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Southwest Conference\n",
    "\n",
    "southwest_url = 'https://en.wikipedia.org/wiki/Southwest_Conference'\n",
    "\n",
    "southwest_df = pd.read_html(southwest_url)\n",
    "\n",
    "southwest_df = southwest_df[3]\n",
    "\n",
    "## save csv (needs to be manually updated)\n",
    "southwest_df.to_csv('TEMP/southwest_conference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Border Conference\n",
    "\n",
    "border_url = 'https://en.wikipedia.org/wiki/Border_Conference'\n",
    "\n",
    "border_df = pd.read_html(border_url)\n",
    "\n",
    "border_final_df = border_df[1]\n",
    "\n",
    "border_former_df = border_df[2]\n",
    "\n",
    "# Add conference id column\n",
    "border_final_df['Conference ID'] = 'Border Conference'\n",
    "border_former_df['Conference ID'] = 'Border Conference'\n",
    "\n",
    "# Save csvs\n",
    "border_final_df.to_csv('TEMP/conference_data/border_conference_final.csv', index=False)\n",
    "border_former_df.to_csv('TEMP/conference_data/border_conference_former.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ivy League\n",
    "\n",
    "ivy_url = 'https://en.wikipedia.org/wiki/Ivy_League'\n",
    "\n",
    "# Read the html table into a dataframe\n",
    "ivy_df = pd.read_html(ivy_url)\n",
    "\n",
    "ivy_current_df = ivy_df[1]\n",
    "\n",
    "# Add conference name to the dataframe\n",
    "ivy_current_df['Conference ID'] = 'Ivy League'\n",
    "\n",
    "## Save the dataframe to a csv file\n",
    "ivy_current_df.to_csv('TEMP/conference_data/ivy_current.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "western_url = 'https://en.wikipedia.org/wiki/Western_Athletic_Conference'\n",
    "\n",
    "# Read the table from the URL\n",
    "western_df = pd.read_html(western_url)\n",
    "\n",
    "# Extract the first table\n",
    "western_current_df = western_df[1]\n",
    "western_former_df = western_df[3]\n",
    "\n",
    "## Add conference id to the western conference\n",
    "western_current_df['Conference ID'] = 'Western Athletic Conference'\n",
    "western_former_df['Conference ID'] = 'Western Athletic Conference'\n",
    "\n",
    "# Save csv files\n",
    "western_current_df.to_csv('TEMP/conference_data/western_current.csv', index=False)\n",
    "western_former_df.to_csv('TEMP/conference_data/western_former.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvc_url = 'https://en.wikipedia.org/wiki/Missouri_Valley_Conference'\n",
    "\n",
    "# Read the html table into a dataframe\n",
    "mvc_df = pd.read_html(mvc_url)\n",
    "\n",
    "mvc_current = mvc_df[2]\n",
    "\n",
    "mvc_former = mvc_df[4]\n",
    "\n",
    "## Add the conference id to the dataframe\n",
    "mvc_current['conference id'] = 'Missouri Valley Conference'\n",
    "mvc_former['conference id'] = 'Missouri Valley Conference'\n",
    "\n",
    "# Save csvs\n",
    "mvc_current.to_csv('TEMP/conference_data/mvc_current.csv', index=False)\n",
    "mvc_former.to_csv('TEMP/conference_data/mvc_former.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape individual conference pages to get conference history\n",
    "\n",
    "conf_usa = 'https://en.wikipedia.org/wiki/Conference_USA'\n",
    "\n",
    "conf_usa_df = pd.read_html(conf_usa)\n",
    "\n",
    "conf_usa_current = conf_usa_df[1] # 1st table is the current members\n",
    "conf_usa_former = conf_usa_df[4] # 2nd table is the former members\n",
    "\n",
    "## Add a column with Conference ID with conference name and - current or former\n",
    "conf_usa_current['ConferenceID'] = 'Conference USA - Current'\n",
    "conf_usa_former['ConferenceID'] = 'Conference USA - Former'\n",
    "\n",
    "## Combine the two dataframes and save\n",
    "conf_usa_df = pd.concat([conf_usa_current, conf_usa_former])\n",
    "conf_usa_df.to_csv('TEMP\\conference_data\\conference_usa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the webpage\n",
    "mac_url = 'https://en.wikipedia.org/wiki/American_Athletic_Conference'\n",
    "response = requests.get(mac_url)\n",
    "page_content = response.content\n",
    "\n",
    "# Step 2: Parse the content using BeautifulSoup\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# Step 3: Identify the tables. Wikipedia tables are often inside 'table' tags with class 'wikitable'\n",
    "tables = soup.findAll('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Clean up problematic attributes (this is just a generic cleanup, specific issues might need more attention)\n",
    "for table in tables:\n",
    "    for cell in table.findAll(['td', 'th']):\n",
    "        if cell.has_attr(\"rowspan\"):\n",
    "            rowspan_value = cell[\"rowspan\"]\n",
    "            # Remove any non-integer characters\n",
    "            cleaned_value = ''.join(filter(str.isdigit, rowspan_value))\n",
    "            if cleaned_value:  # If the cleaned value is not empty\n",
    "                cell[\"rowspan\"] = cleaned_value\n",
    "            else:  # If there's nothing left after cleaning, remove the attribute\n",
    "                del cell[\"rowspan\"]\n",
    "\n",
    "# Step 5: Convert the cleaned tables to pandas DataFrames\n",
    "dfs = []\n",
    "for table in tables:\n",
    "    dfs.append(pd.read_html(str(table))[0])  # Convert each table to a DataFrame\n",
    "\n",
    "# Now, dfs is a list of DataFrames. Each DataFrame corresponds to a table from the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_current = dfs[0]\n",
    "sbc_former = dfs[2]\n",
    "\n",
    "sbc_current['ConferenceID'] = 'Sun Belt Conference - Current'\n",
    "sbc_former['ConferenceID'] = 'Sun Belt Conference - Former'\n",
    "\n",
    "# save csvs\n",
    "sbc_current.to_csv('TEMP\\conference_data\\sun_belt_current.csv', index=False)\n",
    "sbc_former.to_csv('TEMP\\conference_data\\sun_belt_former.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwc_current = dfs[0]\n",
    "mwc_former = dfs[2]\n",
    "\n",
    "dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_current = dfs[0]\n",
    "\n",
    "acc_former = dfs[2]\n",
    "\n",
    "## Add a column with Conference ID with conference name and - current or former\n",
    "acc_current['ConferenceID'] = 'Atlantic Coast Conference - Current'\n",
    "acc_former['ConferenceID'] = 'Atlantic Coast Conference - Former'\n",
    "\n",
    "# save csvs\n",
    "acc_current.to_csv('TEMP\\conference_data\\conference_acc.csv', index=False)\n",
    "acc_former.to_csv('TEMP\\conference_data\\conference_acc_former.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_current = dfs[0]\n",
    "pac_former = dfs[2]\n",
    "\n",
    "dfs[2]\n",
    "\n",
    "# Add a column with Conference ID with conference name and - current or former\n",
    "pac_current['ConferenceID'] = 'Pac-12 - Current'\n",
    "pac_former['ConferenceID'] = 'Pac-12 - Former'\n",
    "\n",
    "# save as csvs\n",
    "pac_current.to_csv('TEMP\\conference_data\\pac_12.csv', index=False)\n",
    "pac_former.to_csv('TEMP\\conference_data\\pac_12_former.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_12_current = dfs[0]\n",
    "\n",
    "big_12_former = dfs[3]\n",
    "\n",
    "## Add a column with Conference ID with conference name and - current or former\n",
    "big_12_current['ConferenceID'] = 'Big 12 - Current'\n",
    "big_12_former['ConferenceID'] = 'Big 12 - Former'\n",
    "\n",
    "## save the dataframes\n",
    "big_12_current.to_csv('TEMP/conference_data/big_12.csv', index=False)\n",
    "big_12_former.to_csv('TEMP/conference_data/big_12_former.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_current = dfs[0]\n",
    "sec_former = dfs[2]\n",
    "\n",
    "sec_current['ConferenceID'] = 'Southeastern Conference - Current'\n",
    "sec_former['ConferenceID'] = 'Southeastern Conference - Former'\n",
    "\n",
    "## save each dataframe to a csv\n",
    "sec_current.to_csv('TEMP\\conference_data\\sec_current.csv', index=False)\n",
    "sec_former.to_csv('TEMP\\conference_data\\sec_former.csv', index=False)\n",
    "\n",
    "# big_ten['ConferenceID'] = 'Big Ten Conference - Current'\n",
    "\n",
    "# big_ten.to_csv('TEMP\\conference_data\\conference_big_ten.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_current = dfs[0]\n",
    "\n",
    "mac_former = dfs[3]\n",
    "\n",
    "## Add a column with Conference ID with conference name and - current or former\n",
    "mac_current['ConferenceID'] = 'Mid-American Conference - Current'\n",
    "mac_former['ConferenceID'] = 'Mid-American Conference - Former'\n",
    "\n",
    "# Save the dataframes\n",
    "mac_current.to_csv('TEMP\\conference_data\\mac_current.csv', index=False)\n",
    "mac_former.to_csv('TEMP\\conference_data\\mac_former.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
